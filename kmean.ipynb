{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b828997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "# -------------------------\n",
    "# Helper Functions\n",
    "# -------------------------\n",
    "def read_csv(filename):\n",
    "    \"\"\"Read a CSV file into a list of lists of floats.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        return [[float(x) for x in row] for row in reader]\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return math.sqrt(sum((a[i] - b[i]) ** 2 for i in range(len(a))))\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    num = sum(a[i] * b[i] for i in range(len(a)))\n",
    "    denom_a = math.sqrt(sum(a[i] ** 2 for i in range(len(a))))\n",
    "    denom_b = math.sqrt(sum(b[i] ** 2 for i in range(len(b))))\n",
    "    if denom_a == 0 or denom_b == 0:\n",
    "        return 1.0\n",
    "    return 1 - (num / (denom_a * denom_b))\n",
    "\n",
    "def generalized_jaccard_distance(a, b):\n",
    "    min_sum = sum(min(a[i], b[i]) for i in range(len(a)))\n",
    "    max_sum = sum(max(a[i], b[i]) for i in range(len(a)))\n",
    "    if max_sum == 0:\n",
    "        return 1.0\n",
    "    return 1 - (min_sum / max_sum)\n",
    "\n",
    "def normalize_data(X):\n",
    "    \"\"\"Normalize dataset values to range [0, 1] using min-max scaling.\"\"\"\n",
    "    # Find global min and max\n",
    "    min_val = min(min(row) for row in X)\n",
    "    max_val = max(max(row) for row in X)\n",
    "    if max_val == min_val:\n",
    "        return X  # avoid division by zero\n",
    "\n",
    "    # Scale each element\n",
    "    normalized = []\n",
    "    for row in X:\n",
    "        normalized.append([(x - min_val) / (max_val - min_val) for x in row])\n",
    "    return normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e333f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# K-Means Implementation\n",
    "# -------------------------\n",
    "class KMeansScratch:\n",
    "    def __init__(self, n_clusters=10, max_iter=100, distance='euclidean', criteria=\"centroid\", random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.stop_method = criteria\n",
    "        random.seed(random_state)\n",
    "        \n",
    "        # Choose distance function\n",
    "        if distance == 'euclidean':\n",
    "            self.dist_func = euclidean_distance\n",
    "        elif distance == 'cosine':\n",
    "            self.dist_func = cosine_distance\n",
    "        elif distance == 'jaccard':\n",
    "            self.dist_func = generalized_jaccard_distance\n",
    "        else:\n",
    "            raise ValueError(\"distance must be 'euclidean', 'cosine', or 'jaccard'\")\n",
    "    \n",
    "    def fit(self, X):\n",
    "        start_time = time.time()\n",
    "        n_features = len(X[0])\n",
    "        current_compute_sse = float('inf')\n",
    "        pre_compute_sse = float('inf')\n",
    "        \n",
    "        # Randomly initialize centroids\n",
    "        self.centroids = random.sample(X, self.n_clusters)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Assign points to clusters\n",
    "            clusters = [[] for _ in range(self.n_clusters)]\n",
    "            for x in X:\n",
    "                distances = [self.dist_func(x, c) for c in self.centroids]\n",
    "                cluster_index = distances.index(min(distances))\n",
    "                clusters[cluster_index].append(x)\n",
    "            \n",
    "            # Compute new centroids\n",
    "            new_centroids = []\n",
    "            for cluster in clusters:\n",
    "                if not cluster:\n",
    "                    # Empty cluster â†’ keep previous centroid\n",
    "                    new_centroids.append(random.choice(X))\n",
    "                else:\n",
    "                    # Mean of cluster points (element-wise)\n",
    "                    new_centroid = []\n",
    "                    for j in range(n_features):\n",
    "                        col_sum = sum(point[j] for point in cluster)\n",
    "                        new_centroid.append(col_sum / len(cluster))\n",
    "                    new_centroids.append(new_centroid)\n",
    "            \n",
    "            self.labels_ = [self._closest_centroid(x) for x in X]\n",
    "            current_compute_sse = self._compute_sse(X)\n",
    "            # Check for convergence\n",
    "            if self._converged(self.centroids, new_centroids,\n",
    "                           current_sse=current_compute_sse, prev_sse=pre_compute_sse,\n",
    "                           iteration=iteration, max_iter=self.max_iter):\n",
    "                \n",
    "                break\n",
    "\n",
    "            pre_compute_sse = current_compute_sse\n",
    "            \n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        # Store final cluster assignments and compute SSE\n",
    "        \n",
    "        self.inertia_ = current_compute_sse\n",
    "\n",
    "        self.iterations_ = iteration + 1\n",
    "        self.time_ = time.time() - start_time\n",
    "    \n",
    "    def _closest_centroid(self, x):\n",
    "        distances = [self.dist_func(x, c) for c in self.centroids]\n",
    "        return distances.index(min(distances))\n",
    "    \n",
    "    def _compute_sse(self, X):\n",
    "        sse = 0.0\n",
    "        for i, x in enumerate(X):\n",
    "            centroid = self.centroids[self.labels_[i]]\n",
    "            dist = self.dist_func(x, centroid)\n",
    "            sse += dist ** 2\n",
    "        return sse\n",
    "    \n",
    "    def _converged(self, old_centroids, new_centroids, current_sse=None, prev_sse=None,\n",
    "               iteration=None, max_iter=None, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Determine if K-Means should stop:\n",
    "        1. Centroid movement below tolerance\n",
    "        2. SSE stops improving (increasing)\n",
    "        3. Reached maximum iteration count\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Centroid-based stopping ---\n",
    "        total_change = 0.0\n",
    "        for i in range(len(old_centroids)):\n",
    "            total_change += euclidean_distance(old_centroids[i], new_centroids[i])\n",
    "        if self.stop_method == 'centroid' and total_change < tol:\n",
    "            print(f\"Converged at iteration {iteration + 1} (centroids stabilized).\")\n",
    "            return True\n",
    "\n",
    "        # --- SSE-based stopping ---\n",
    "        if self.stop_method == 'sse' and current_sse is not None and prev_sse is not None:\n",
    "            if current_sse >= prev_sse:\n",
    "                print(f\"Stopping early at iteration {iteration + 1} (SSE stopped improving).\")\n",
    "                return True\n",
    "\n",
    "        # --- Maximum iteration stopping ---\n",
    "        if (self.stop_method == \"max_iteration\") & (max_iter is not None and iteration + 1 >= max_iter):\n",
    "            print(f\"Reached maximum iterations ({max_iter}). Stopping.\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab3e4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_cluster_accuracy(labels, true_labels, k):\n",
    "    \"\"\"\n",
    "    Compute clustering accuracy using majority voting within each cluster.\n",
    "    labels: predicted cluster IDs (from KMeans)\n",
    "    true_labels: ground-truth class labels\n",
    "    k: number of clusters\n",
    "    \"\"\"\n",
    "    cluster_to_label = {}\n",
    "    correct = 0\n",
    "\n",
    "    for cluster_id in range(k):\n",
    "        cluster_indices = [i for i, lbl in enumerate(labels) if lbl == cluster_id]\n",
    "        if not cluster_indices:\n",
    "            continue\n",
    "        cluster_true_labels = [true_labels[i] for i in cluster_indices]\n",
    "        most_common = Counter(cluster_true_labels).most_common(1)[0][0]\n",
    "        cluster_to_label[cluster_id] = most_common\n",
    "\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        predicted_label = cluster_to_label.get(cluster_id, -1)\n",
    "        if predicted_label == true_labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5e1add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples: 1000\n",
      "Features per sample: 784\n",
      "Data normalized to [0, 1]\n",
      "----------------------------------------\n",
      "Running K-Means with euclidean \n",
      "Converged at iteration 18 (centroids stabilized).\n",
      "Euclidean SSE: 37082.3819\n",
      "Euclidean K-Means Accuracy: 59.60%\n",
      "Iterations: 18\n",
      "Runtime: 18.64 seconds\n",
      "----------------------------------------\n",
      "Running K-Means with cosine \n",
      "Converged at iteration 23 (centroids stabilized).\n",
      "Cosine SSE: 74.3334\n",
      "Cosine K-Means Accuracy: 59.50%\n",
      "Iterations: 23\n",
      "Runtime: 47.43 seconds\n",
      "----------------------------------------\n",
      "Running K-Means with jaccard \n",
      "Converged at iteration 23 (centroids stabilized).\n",
      "Jaccard SSE: 382.3725\n",
      "Jaccard K-Means Accuracy: 56.30%\n",
      "Iterations: 23\n",
      "Runtime: 54.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Main Execution\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data = read_csv('data.csv')\n",
    "    labels = [int(x[0]) for x in read_csv('label.csv')]\n",
    "\n",
    "    use_subdata = 1000\n",
    "    \n",
    "    print(\"Data samples:\", use_subdata)\n",
    "    print(\"Features per sample:\", len(data[0]))\n",
    "\n",
    "    # Normalize data\n",
    "    normalized_data = normalize_data(data[:use_subdata])\n",
    "    print(\"Data normalized to [0, 1]\")\n",
    "    \n",
    "    subset_labels = labels[:use_subdata]\n",
    "    \n",
    "\n",
    "    for metric in ['euclidean', 'cosine', 'jaccard']:\n",
    "        print(\"-\"*40)\n",
    "        print(f\"Running K-Means with {metric} \")\n",
    "\n",
    "        # there are three criteria: \"centroid\", \"sse\", \"max_iteration\"\n",
    "        kmeans = KMeansScratch(n_clusters=10, max_iter=35, distance=metric, criteria=\"centroid\")\n",
    "        kmeans.fit(normalized_data)\n",
    "\n",
    "        print(f\"{metric.capitalize()} SSE: {kmeans.inertia_:.4f}\")\n",
    "\n",
    "        acc = compute_cluster_accuracy(kmeans.labels_, subset_labels, 10)\n",
    "        print(f\"{metric.capitalize()} K-Means Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "        print(f\"Iterations: {kmeans.iterations_}\")\n",
    "        print(f\"Runtime: {kmeans.time_:.2f} seconds\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28e3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
